#### Ansible
1. 将跳板机物料包发送到管控主机` scp /home/hjwtest/* root@172.16.14.147:/root/data`
2. 解压ansible压缩包，给安装脚本添加可执行权限 `  tar -zxvf ansible_r1813.tar.gz && chmod u+x ansible/install.sh `
3. 配置hosts文件

    > ```
    > [all:vars]
    > ansible_port=22
    > ansible_ssh_user=root
    > ansible_ssh_pass=hycloud@123
    > [dbs]
    > 172.16.14.147
    > 172.16.14.148
    > 172.16.14.149
    > 
    > [kvs]
    > 172.16.14.151
    > 172.16.14.152
    > 172.16.14.157
    > 
    > [MQ]
    > 172.16.14.148  
    > 172.16.14.149
    > 
    > [memcache]
    > 172.16.14.147
    > ```
#### zookeeper 3.4.6
- 修改 tdsql_zk下的zkip.txt ntp.txt
- 上传安装包到目标主机
  > `ansible dbs -m copy -a 'src=/root/data/tdsql_zk dest=/root/'`    `ansible kvs -m copy -a 'src=/root/data/tdsql_zk dest=/root/'`
- 将zkip.txt内容添加进/etc/hosts（不然会通信失败）zoo.cfg里面的域名必须能够被主机解析 `ansible dbs -m shell -a 'cat /root/tdsql_zk/zkip.txt >> /etc/hosts'`
- 安装Java8.5
  > ` ansible dbs -m shell -a 'rpm -ivh /root/tdsql_zk/jdk-8u5-linux-x64.rpm' `     ` ansible kvs -m shell -a 'rpm -ivh /root/tdsql_zk/jdk-8u5-linux-x64.rpm' `
- 检查环境，如需要用到的变量是否配好，端口是否被占用，目录是否存在等等
- 执行install_zk.sh安装zk
  > `ansible dbs -m shell -a 'bash /root/tdsql_zk/install_zk.sh'`     `ansible kvs -m shell -a 'bash /root/tdsql_zk/install_zk.sh'`
- 检查状态 `ansible dbs -m shell -a '/data/zookeeper-3.4.6/bin/zkServer.sh status'`

#### hdfs
- 环境初始化 修改主机名，并配置添加好hosts解析(playbook中有这一步)    
    ~~vi /root/hosts.tmp~~v    
    ~~v172.16.14.147    hdfsnamenode1~~    
    ~~v172.16.14.148    hdfsnamenode2~~     
    ~~172.16.14.149   hdfsdatanode~~
  > 将tmp文件发送至各主机 `ansible dbs -m copy -a 'src=/root/data/hosts.tmp dest=/root'`
  > 将tmp文件内容追加至/etc/hosts ` ansible dbs -m shell -a 'cat /root/hosts.tmp >> /etc/hosts' `
  > 检查是否添加成功
- 安装hdfs
  1. 在管控主机上将hdfs安装包移至/data下
  2. 进入/data并解压压缩包` cd /data&&tar -zvxf hdfs-2.6.5.tar.gz `
  3. cd /data/hdfs/scripts
  4. 配置hdfs_hosts文件
  5. 修改datanode存储路径变量 `vi vars.yml`

### CDB(OSS) 安装
- `ip a show ${name} 2>/dev/null |grep -Eo 'inet (addr:)?([0-9]*\.){3}[0-9]*'|grep -v "127.0."`
- ` ip -4 route get 8.8.8.8|grep -v cache|awk '{print $7}'`
- **<font color=red>执行之前先规划好个应用与主机之间的关联</font>**
- 将cdb_worker.tar.gz发送至各个 **<font color=red>(所有相关主机都需要安装)</font>** 节点/data目录下，解压缩并执行cdb_worker/tools/op/install.sh进行安装
- 将 cdb_auto_deploy.v0208.a347b9e.tar.g发送至 master 和 slave主机 /data目录下
- 在master 和 slave主机上安装rsync (yum install -y rsync)
- 所有使用ipconfig的脚本都需要重构使用操作系统自带的ip命令，减少对第三方软件的依赖
- mysql安装时需要perl环境以及相关模块(yum install -y perl perl-Module*)
- mysql 安装时报错缺少 libaio.so.1 (yum install -y libaio-devel libaio)
- mysql启动命令执行后，脚本内的需要sleep几秒钟，避免服务还没起来会报错
- 安装
  - `./one_key_install_oss_master.sh -n3 -t'remote' -i'172.16.14.151 172.16.14.152 172.16.14.157' -o128 -p'172.16.14.148 172.16.14.149' -r'pcdb_test' -a'172.16.14.149' -y'172.16.14.148:8888' ` master
  - `./one_key_install_oss_slave.sh -n3 -t'remote' -i'172.16.14.151 172.16.14.152 172.16.14.157' -o128 -p'172.16.14.148 172.16.14.149' -r'pcdb_test' -m'172.16.14.148' -y'172.16.14.148:8888' `
  - `cd /data/cdb_auto_deploy/install_sh &&  ./one_key_uninstall.sh  -p '172.16.14.148 172.16.14.149'  -i '172.16.14.151 172.16.14.152 172.16.14.157'` 安装失败时执行，清理环境


#### Kafka安装
- 前置依赖项(java zk)都已安装,zk集群使用的是（172.16.14.151,172.16.14.152,172.16.14.157)
- 发送物料包到目的主机, ` ansible dbs -m copy -a 'src=/root/data/kafka dest=/data' `
- 由于前置依赖已安装，所以直接将kafka压缩包解压到/usr/local/services/ `ansible dbs -m shell -a 'tar -zxvf /data/kafka/raw/kafka_2.11-0.9.0.1.tar.gz -c /usr/local/services/' `
- 将zk域名与主机对应关系写入/etc/hosts文件
  1. vi /root/barad.tmp
```
zk1.barad 172.16.14.151
zk2.barad 172.16.14.152
zk3.barad 172.16.14.157
```
  2. `ansible dbs -m copy -a 'src=/root/barad.tmp dest=/root/' `
  3. `ansible dbs -m shell -a 'cat /root/barad.tmp >> /etc/hosts' `
- 修改conf目录下server.properties
  > broker.id
  > host.name (local_ip)
  > zookeeper.connect :=zk1.barad:2181,zk2.barad:2181,zk3.barad:2181/kafka-barad-event
- 执行zkCli.sh进入交互式界面执行(执行一次就行) `create /kafka-barad-event ""` 建立节点
- 在kafka安装目录下执行 `./bin/kafka-server-start.sh -daemon ../config/server.properties` 启动kafka

-----
- 验证(所有命令均在安装目录执行)
  1. 创建topic `./bin/kafka-topics.sh --create --zookeeper zk1.barad:2181/kafka-barad-event --replication-factor 2 -partitions 1 --topic test`
  2. 若创建成功则可以查看topic状体 `./bin/kafka-topics.sh --describe --zookeeper zk1.barad:2181/kafka-barad-event`


#### RabbitMQ安装
- 将物料包发送到目的主机(包含erlang rabbitmq)
- 安装erlang时需要使用yum
- 安装cdb_mysql之后yum失效报错(pycurl.so: undefined symbol: CRYPTO_num_locks)查看`ldconfig -v | grep libcurl`发现libcurl.so.4指向多个源。修改/etc/ld.so.conf删除多余lib之后执行ldconfig可以成功使用yum

#### ES
- cd /data/es/raw/repository &&  sh cfd_pkg.sh c_log 123 private "" /data1 /data/c_log 1.8.0_91 jre-8u91-linux-x64.rpm
- 报错(sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory) 解决:执行modprobe br_netfilter
##### etcd
```
cd /data/c_log/repository/etcd && nohup ./etcd --name infra0 --initial-advertise-peer-urls http://172.16.14.151:2380 \
  --listen-peer-urls http://172.16.14.151:2380 \
  --listen-client-urls http://172.16.14.151:2379,http://127.0.0.1:2379 \
  --advertise-client-urls http://172.16.14.151:2379 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster \
infra0=http://172.16.14.151:2380,infra1=http://172.16.14.152:2380,infra2=http://172.16.14.157:2380 \
  --initial-cluster-state new &
```
```
cd /data/c_log/repository/etcd && nohup ./etcd --name infra1 --initial-advertise-peer-urls http://172.16.14.152:2380 \
  --listen-peer-urls http://172.16.14.152:2380 \
  --listen-client-urls http://172.16.14.152:2379,http://127.0.0.1:2379 \
  --advertise-client-urls http://172.16.14.152:2379 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster infra0=http://172.16.14.151:2380,infra1=http://172.16.14.152:2380,infra2=http://172.16.14.157:2380 \
  --initial-cluster-state new &

```
```
cd /data/c_log/repository/etcd && nohup ./etcd --name infra2 --initial-advertise-peer-urls http://172.16.14.157:2380 \
  --listen-peer-urls http://172.16.14.157:2380 \
  --listen-client-urls http://172.16.14.157:2379,http://127.0.0.1:2379 \
  --advertise-client-urls http://172.16.14.157:2379 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster  infra0=http://172.16.14.151:2380,infra1=http://172.16.14.152:2380,infra2=http://172.16.14.157:2380 \
  --initial-cluster-state new &

```
